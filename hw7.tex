%% Standard start of a latex document
\documentclass[letterpaper,12pt]{article}
%% Always use 12pt - it is much easier to read
%% Things written after '%' sign, are ignored by the latex editor - they are how to introduce comments into your .tex source
%% Anything mathematics related should be put in between '$' signs.

%% Set some names and numbers here so we can use them below
\newcommand{\name}{James Wu} %%%%%%%%%%%%%%% ---------> Change this to your name
\newcommand{\studentnumber}{92277235} %%%%%%%%%%%%%%% ---------> Change this to your student number
\newcommand{\hwnum}{7} %%%%%%%%%%%%%%% --------->  set this to the homework number

%%%%%%
%% There is a bit of stuff below which you should not have to change
%%%%%%

%% AMS mathematics packages - they contain many useful fonts and symbols.
\usepackage{amsmath, amsfonts, amssymb, bm, amsthm}

%% The geometry package changes the margins to use more of the page, I suggest
%% using it because standard latex margins are chosen for articles and letters,
%% not homework.
\usepackage[paper=letterpaper,left=25mm,right=25mm,top=30mm,bottom=30mm]{geometry}
%% For details of how this package work, google the ``latex geometry documentation''.

%% Fancy headers and footers - make the document look nice
\usepackage{fancyhdr} %% for details on how this work, search-engine ``fancyhdr documentation''
\pagestyle{fancy}

\usepackage{graphicx}

\setlength{\headheight}{15pt}

%% The header
\lhead{MATH 318} % course name as top-left
\chead{Homework \hwnum} % homework number in top-centre
\rhead{ \name, \studentnumber }
%% This is a little more complicated because we have used `` \\ '' to force a line-break between the name and number.

%% The footer
\lfoot{\name} % name on bottom-left
\cfoot{Page \thepage} % page in middle
\rfoot{\studentnumber} % student number on bottom-right

%% These put horizontal lines between the main text and header and footer.
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
%%%

%%%%%%
%% The above stuff is the same as the first template, but now we are starting to prove things, so we'd like to have a
%% good proof environment that gives us nice formatting and a little square at the end.
%% We'd also like a nice Result environment that prints that up nicely too.
%% Thankfully this exists in latex in the amsthm package
\usepackage{amsthm}
\newtheorem*{thm}{Theorem}
%% This creates a new theorem-like environment called "result", that will be titled "Result".
%% See below for examples of how to use this.
%%%%%%
\usepackage{enumitem}
%% This package allows us to make nice ordered lists with numbers, letters or roman numerals

\usepackage{titlesec}
\titlespacing*{\subsection}{0pt}{0pt}{3.0ex}
\titlespacing*{\subsubsection}{0pt}{3.0ex}{0.5ex}

\usepackage[hang,flushmargin]{footmisc}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\allowdisplaybreaks

\usepackage{empheq}

\newcommand*\wfbox[1]{\fbox{\hspace{0.4em}#1\hspace{0.4em}}}

%% Useful commands
\renewcommand*{\qed}{\hfill\ensuremath{\square}}

\newcommand*{\uvec}[1]{\hat{\bm{#1}}}

\newcommand*{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand*{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand*{\nderiv}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand*{\npderiv}[3]{\frac{\partial^{#3} #1}{\partial #2^{#3}}}
\newcommand*{\divg}[1]{\nabla \cdot \mathbf{#1}}
\newcommand*{\curl}[1]{\nabla \times \mathbf{#1}}

\newcommand*{\abs}[1]{\left| #1 \right|}
\newcommand*{\norm}[1]{\abs{\abs{\mathbf{#1}}}}

\newcommand*{\ev}[1]{\left<#1\right>}

\renewcommand*{\Re}[1]{\text{Re}\left(#1\right)}
\renewcommand*{\Im}[1]{\text{Im}\left(#1\right)}

\newcommand*{\qimg}[2]{\\ \begin{center}\includegraphics[scale=#1]{#2}\end{center}}

\newcommand*{\Arg}[1]{\text{Arg}\left(#1\right)}

\newcommand*{\Log}[1]{\text{Log}\left(#1\right)}

\newcommand*{\Binom}[2]{\text{Binom}\left(#1, #2\right)}
\newcommand*{\Geom}[1]{\text{Geom}\left(#1\right)}
\newcommand*{\Poisson}[1]{\text{Poisson}\left(#1\right)}
\newcommand*{\Var}[1]{\text{Var}\left(#1\right)}
\newcommand*{\Cov}[2]{\text{Cov}\left(#1, #2\right)}

\newcommand*{\cdf}[2]{P\left(#1 \leq #2\right)}
\newcommand*{\erv}[1]{\mathbb{E}\left[#1\right]}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}[theorem]

%%

\begin{document}
\begin{flushleft}

    \subsection*{MATH 318 Homework \hwnum}

    \subsubsection*{Problem 1}
    \begin{enumerate}[label=(\alph*)]

        \item
        The event $E$ that the walker returns to the origin in $2n$ steps is precisely the event that there are equally many left and right steps, as well as equally many up and down steps. Let $E_k$ be the event that there are $k$ left and right steps, and consequently $n - k$ up and down steps. Notice that we require $k \geq 0$ and $n - k \geq 0 \, \therefore \, k \leq n$. It follows that $E_i \cap E_j = \emptyset$ for $i \neq j$ and $E = \cup_{k=0}^n E_k$.

        By symmetry, each permutation of steps is equally likely. There are $\frac{1}{4^{2n}}$ total permutations, since there are $2n$ steps with four options per step. The number of permutations in the event $E_k$ is simply the number of ways to arrange $k$ left and right steps, and $n - k$ up and down steps. The multinomial coefficient gives us
        $$\frac{(2n)!}{(k!)^2((n-k)!)^2} = \left(\frac{(2n)!}{(n!)^2}\right)\left(\frac{(n!)^2}{(k!)^2((n-k)!)^2}\right) = \binom{2n}{n}\binom{n}{k}^2$$
        
        Thus
        $$P(E_k) = \frac{1}{4^{2n}}\binom{2n}{n}\binom{n}{k}^2$$

        By Kolmogorov's third axiom of probability,
        \begin{align*}
            p_{2n} &= P(E) = \sum_{k=0}^n P(E_k) = \frac{1}{4^{2n}}\binom{2n}{n} \sum_{k=0}^n\binom{n}{k}^2
        \end{align*}
        \qed
        
        \item
        As a small flex, we take a different approach from the hint. Consider the polynomial
        $$(1 + x)^{2n} = (1 + x)^n (1 + x)^n$$
        The coefficient on the $x^n$ term is $\binom{2n}{n}$. Meanwhile, the coefficient on the $x^k$ term for the polynomial $(1 + x)^n$ is $\binom{n}{k}$. But by multiplying $(1 + x)^n$ with itself, we find that the coefficient on the $x^n$ term is also given by the convolutional sum
        $$\binom{2n}{n} x^n = \sum_{k=0}^n \left(\binom{n}{k} x^k\right)\left(\binom{n}{n-k} x^{n-k}\right) = \sum_{k=0}^n \binom{n}{k}\binom{n}{n-k}x^n$$
        $$\binom{2n}{n} = \sum_{k=0}^n \binom{n}{k}\binom{n}{n-k} = \sum_{k=0}^n \binom{n}{k}^2$$
        \qed

        \item
        We use the identity from part (b) to find
        $$P(E) = \frac{1}{4^{2n}}\binom{2n}{n}^2$$

        Let $M$ be the number of returns to the origin. If we denote $X_{2n}$ as an indicator RV for returning to the origin after $2n$ steps, we have by linearity of expectaion
        \begin{align*}
            \erv{M} &= \sum_{n=0}^\infty \erv{X_{2n}} = \sum_{n=0}^\infty p_{2n} = \sum_{n=0}^\infty \frac{1}{4^{2n}}\binom{2n}{n}^2
        \end{align*}
        The convergence of this sum depends only on the behaviour of the summand for large $n$. In this regime, we use Stirling's approximation:
        \begin{align*}
            \erv{M} &\sim \sum_{n=0}^\infty \frac{1}{4^{2n}} \left(\frac{(2n)^{2n} e^{-2n} \sqrt{4\pi n}}{n^{2n} e^{-2n} (2\pi n)}\right)^2 = \sum_{n=0}^\infty \frac{1}{4^{2n}} \left(\frac{2^{2n}}{\sqrt{\pi n}}\right)^2 = \sum_{n=0}^\infty \frac{1}{\pi n}
        \end{align*}
        This is a harmonic series, which diverges to infinity. Hence $\erv{M} = +\infty$, so the transient walk is recurrent.\qed

    \end{enumerate}

    \subsubsection*{Problem 2}
    Let $Y_i$ be the indicator RV that the $i$th sampled ball is black. For a given $X$, the probability of this occuring is then $X/8$. Now notice that $Y = \sum_{i=1}^10 Y_i$. By linearity of expectation,
    $$\boxed{\erv{Y | X} = \sum_{i=1}^{10} \frac{X}{8} = \frac{5}{4}X}$$
    Since $Y|X$ is the sum of 10 Bernoulli trials, we have
    $$P(Y = y|X = x) \sim \Binom{10}{x/8} = \left(\frac{x}{8}\right)^y \left(\frac{8 - x}{8}\right)^{10 - y} \binom{10}{y}$$
    Since $X$ is uniform, $P(X = x) = 1/9$ for $0 \leq x \leq 8$. By Bayes' theorem,
    \begin{align*}
        P(X = x | Y = y) &= \frac{P(Y = y|X = x)P(X = x)}{\sum_{k=0}^8 P(Y = y|X = k) P(X = k)} = \frac{\left(\frac{x}{8}\right)^y \left(\frac{8 - x}{8}\right)^{10 - y}}{\sum_{k=0}^8 \left(\frac{k}{8}\right)^y \left(\frac{8 - k}{8}\right)^{10 - y}} \\
        &= \frac{x^y (8 - x)^{10 - y}}{\sum_{k=0}^8 k^y (8 - k)^{10 - y}} \\
        \erv{X|Y} &= \sum_{x=0}^8 x\frac{x^y (8 - x)^{10 - y}}{\sum_{k=0}^8 k^y (8 - k)^{10 - y}}
    \end{align*}
    $$\boxed{\erv{X|Y} = \frac{\sum_{k=0}^8 k^{y+1} (8 - k)^{10 - y}}{\sum_{k=0}^8 k^y (8 - k)^{10 - y}}}$$

    \subsubsection*{Problem 3}
    \begin{enumerate}[label=(\alph*)]

        \item
        Nonzero transistion probabilities are described graphically as follows:
        \qimg{0.8}{img/q3a.jpg}
        Since the states $\{1, 2, 3\}$ and $\{4, 5\}$ are in different communicating classes, the Markov chain is $\boxed{\text{not irreducible}}$. Looking at the graph, we also characterize each state:
        $$\boxed{\begin{cases}
            \text{1: recurrent and periodic with period 2} \\
            \text{2: transient and aperiodic} \\
            \text{3: recurrent and periodic with period 2} \\
            \text{4: transient and aperiodic} \\
            \text{5: recurrent and aperiodic}
        \end{cases}}$$

        \item
        Nonzero transistion probabilities are described graphically as follows:
        \qimg{0.8}{img/q3b.jpg}
        Because we have the loop $1 \rightarrow 2 \rightarrow 3 \rightarrow 4 \rightarrow 5 \rightarrow 1$, the Markov chain is $\boxed{\text{irreducible}}$. Hence, $\boxed{\text{all states are recurrent}}$. Finally, notice that $P_{11}^4 > 0$ because we have the chain $1 \rightarrow 3 \rightarrow 4 \rightarrow 5 \rightarrow 1$ but also $P_{11}^5 > 0$ because of the chain $1 \rightarrow 2 \rightarrow 3 \rightarrow 4 \rightarrow 5 \rightarrow 1$. Since 4 and 5 are relatively prime, this state is aperiodic. But because the Markov chain is irreducible, $\boxed{\text{all states are aperiodic}}$.

        \item
        Every pair of states is communicating. To see how, we consider the path taken by incrementing/decrementing a single dimension's coordinate, repeating for each dimension, until the target node is reached from the source node. This path has a probability of $\frac{1}{(2d)^M} > 0$, where $M$ is the Manhattan distance between the two points. The transition probability $P_{ij}^M$ between these two states over $M$ steps is therefore at least $\frac{1}{(2d)^M} > 0$. Hence $\boxed{\text{all symmetric random walks are irreducible}}$. The origin is recurrent for $d \leq 2$ and transient otherwise. Since recurrence is a class property, $\boxed{\text{all states are reccurent for } d \leq 2 \text{ and transient otherwise}}$. Finally, returning to a given node requires an even number of steps. To see why, notice that every step flips the parity of the sum of all coordinates. Furthermore, it is possible to return to a state by moving to the right and back i.e. in 2 steps. Thus $\boxed{\text{all states are periodic with period 2}}$.

        \item
        Every pair of states is communicating. Suppose we have coordinates $i < j$. Then we can go from $i$ to $j$ in $j - i$ steps with nonzero probability $\left(\frac{2}{3}\right)^{j - i}$ and go from $j$ to $i$ in $j - i$ steps with nonzero probability $\left(\frac{1}{3}\right)^{j - i}$. Thus the Markov chain is $\boxed{\text{irreducible}}$. Furthermore, $\boxed{\text{every state is transient}}$. To see why, recall that the asymmetric random walk on $\mathbb{Z}$ has a finite expected number of returns to the origin. Because the Markov chain is irreducible, every state must be transient like the origin is. Finally, we may employ the parity logic used in part (c) to find that $\boxed{\text{all states are periodic with period 2}}$.

    \end{enumerate}

\end{flushleft}
\end{document}
